{"cells":[{"cell_type":"markdown","metadata":{"id":"YSbtUgtcWsHc"},"source":["## Dependency Parsing as a Preprocessing Step for Logical Reasoning"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pT7Ba6R-zFLz","executionInfo":{"status":"ok","timestamp":1639604452294,"user_tz":300,"elapsed":4392,"user":{"displayName":"elbowroom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16478181345569282121"}},"outputId":"74ec6e3d-772d-4bf9-f39e-7a319ca601cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["# Installs HBOX for Jupyer Notebooks\n","# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\n","# !jupyter nbextension enable --py widgetsnbextension\n","# drive_path = ''\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","drive_path = '/content/drive/MyDrive/FinalProject/'\n","!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qGQfwNPzFL0","executionInfo":{"status":"ok","timestamp":1639604461426,"user_tz":300,"elapsed":9136,"user":{"displayName":"elbowroom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16478181345569282121"}},"outputId":"c27dac47-8332-4bee-e0d8-eec245b538df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Dec 15 21:41:01 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["import os\n","import gc\n","import re\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import RobertaTokenizer\n","from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import RobertaConfig, RobertaForSequenceClassification, AdamW\n","\n","# Enable CUDA Blocking Debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","CUDA_LAUNCH_BLOCKING=\"1\"\n","\n","# Print GPU Information\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","    print('and then re-execute this cell.')\n","else:\n","    print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"GrSiTjIeXaFn"},"source":["#### Load Data"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dN0fYjnBWmx-","executionInfo":{"status":"ok","timestamp":1639604461427,"user_tz":300,"elapsed":6,"user":{"displayName":"elbowroom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16478181345569282121"}}},"outputs":[],"source":["def load_data(path, test):\n","    df = pd.DataFrame({'prompt': [], 'label': []})\n","    data = json.load(open(path))\n","    for i in range(len(data['context'])):\n","        # Add BERT tokens to prompt\n","        prompt = data['context'][str(i)] + ' </s> ' + data['question'][str(i)] + ' </s> '# + data['dep_context'][str(i)] + ' </s> '\n","        for j in range(4):\n","            # Add BERT tokens to answer\n","            answer = data['answers'][str(i)][j]# + ' <d> ' + data['dep_answers'][str(i)][j] + ' <s>'\n","            \n","            # Attach 0, 1 label as array\n","            label = [1] if not test and j == data['label'][str(i)] else [0]\n","            \n","            # Append question, answer pair to dataframe\n","            df = df.append({'prompt': prompt + answer, 'label': [label], 'dep_tree': data['dep_context'][str(i)]}, ignore_index=True)            \n","    return df"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"RMPT4GNOzFL2","executionInfo":{"status":"ok","timestamp":1639604517900,"user_tz":300,"elapsed":56478,"user":{"displayName":"elbowroom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16478181345569282121"}}},"outputs":[],"source":["train_data = load_data(drive_path + 'reclor_data_with_dependencies/train.json', False)\n","val_data = load_data(drive_path + 'reclor_data_with_dependencies/val.json', False)\n","test_data = load_data(drive_path + 'reclor_data_with_dependencies/test.json', True)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJJXNtvyzFL2","executionInfo":{"status":"ok","timestamp":1639604518236,"user_tz":300,"elapsed":340,"user":{"displayName":"elbowroom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16478181345569282121"}},"outputId":"f074c74b-8776-41b9-e6e0-fd482a9c2fec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    18552.000000\n","mean       103.285630\n","std         23.085243\n","min         36.000000\n","25%         88.000000\n","50%        102.000000\n","75%        116.000000\n","max        230.000000\n","Name: prompt, dtype: float64"]},"metadata":{},"execution_count":5}],"source":["# Describe the token lengths of training data\n","train_data['prompt'].apply(lambda x: len(re.findall(r'\\w+', x))).describe()"]},{"cell_type":"markdown","metadata":{"id":"QewmF4FjXdMI"},"source":["#### Initialize tokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FifoAfYWmyC","outputId":"6a7faa3c-d698-40f7-b05a-5d1fa17c2934","executionInfo":{"status":"ok","timestamp":1639604521597,"user_tz":300,"elapsed":3363,"user":{"displayName":"elbowroom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16478181345569282121"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":6}],"source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","tokenizer.add_special_tokens({'additional_special_tokens': ['<d>']})"]},{"cell_type":"markdown","metadata":{"id":"ST4vzaugXe3F"},"source":["#### Tokenize the data"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_IwKvRzrWmyD","executionInfo":{"status":"ok","timestamp":1639604521598,"user_tz":300,"elapsed":5,"user":{"displayName":"elbowroom","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16478181345569282121"}}},"outputs":[],"source":["# Creates a dataloader (which includes an attention mask)\n","def preprocess(in_, tokenizer, max_len, batch_size, data_class='train'):\n","    encoded_input = tokenizer(in_['prompt'].values.tolist(), padding=True, max_length=max_len, truncation=True, return_tensors=\"pt\")\n","    \n","    if data_class != 'test':\n","        labels = torch.tensor(in_['label'].values.tolist())\n","    \n","    dep_tree = in_['dep_tree'].values.tolist()\n","    dep_mask = np.array([]) \n","    for i in tqdm(range(int(len(dep_tree) / 4))):\n","        masks = np.array([])\n","        for j in range(batch_size):\n","          dep = dep_tree[i + j]\n","          dependencies = dep.split('],')\n","          m = np.zeros([768, 768])\n","          for i in range(len(dependencies)):\n","              for j in range(len(dependencies)):\n","                  for x in dependencies[i]:\n","                      if dependencies[j][0] in x:\n","                          m[i][j] = 1\n","                      else:\n","                          m[i][j] = 0\n","          np.append(masks, m)\n","        np.append(dep_mask, masks)\n","    dep_mask = torch.tensor(dep_mask, dtype=torch.float)\n","    #print(dep_mask.shape)\n","\n","    dataset_tensor = TensorDataset(encoded_input['input_ids'], encoded_input['attention_mask'], labels, dep_mask)\n","    sampler = SequentialSampler(dataset_tensor)\n","    #sampler = RandomSampler(dataset_tensor) if data_class == \"train\" else SequentialSampler(dataset_tensor)\n","    dataloader = DataLoader(dataset_tensor, sampler=sampler, batch_size=batch_size)\n","    return dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IO_u0WCuzFL5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3f6e1a3-d638-41a1-ac51-7a087eccd531"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 57%|█████▋    | 2664/4638 [18:33<11:51,  2.77it/s]"]}],"source":["max_len = 512 # should be 1024\n","batch_size = 4\n","train_dataloader = preprocess(train_data, tokenizer, max_len, batch_size)\n","val_dataloader = preprocess(val_data, tokenizer, max_len, batch_size, data_class=\"val\")\n","# test_dataloader = preprocess(test_data, tokenizer, max_len, batch_size, data_class=\"test\")"]},{"cell_type":"markdown","metadata":{"id":"BdmnMX7OXhRJ"},"source":["#### Train and Evaluate RoBERTa"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-Uf7cPZWmyE"},"outputs":[],"source":["def ClearTorch():\n","    torch.no_grad()\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","def Eval(model, dataloader):\n","    ClearTorch()\n","    model.eval()\n","    predictions, true_labels = [], []\n","    for step, batch in enumerate(tqdm(dataloader)):\n","        # Call model on batch\n","        input_ids, attention_mask, labels, dep_mask = batch[0].cuda(), batch[1].cuda(), batch[2].cuda(), batch[3].cuda()\n","        outputs = model(input_ids, dep_mask=dep_mask, attention_mask=attention_mask, labels=labels)\n","        \n","        # Convert output logit to predictions using softmax\n","        #print(labels)\n","\n","        predictions.append(torch.nn.functional.softmax(outputs.logits).argmax(0)[1].cpu().numpy().tolist())\n","        true_labels.append(labels.argmax(0).cpu().numpy().tolist()[0][0])\n","      \n","        ClearTorch()\n","\n","    return float(sum([predictions[i] == true_labels[i] for i in range(len(predictions)) ])) / float(len(predictions))\n","\n","def Train(model, train_data, lr, n_epoch):\n","    ClearTorch()\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","\n","    for epoch in range(n_epoch):\n","        print(f\"Epoch {epoch}\")\n","        model.train()\n","        nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n","\n","        for step, batch in enumerate(tqdm(train_data)):\n","            # RoBERTa fine-tuning\n","            input_ids, attention_mask, labels, dep_mask = batch[0].cuda(), batch[1].cuda(), batch[2].cuda(), batch[3].cuda()\n","            \n","            optimizer.zero_grad()\n","            outputs = model(input_ids, dep_mask = dep_mask, attention_mask=attention_mask, labels=labels)\n","            outputs.loss.backward()\n","            optimizer.step()\n","            \n","            ClearTorch()\n","            \n","            tr_loss += float(outputs.loss)\n","            nb_tr_steps += 1\n","            \n","        print(f\"Train loss on epoch {epoch}: {tr_loss / nb_tr_steps}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtqGmktQzFL6"},"outputs":[],"source":["ClearTorch()\n","\n","config = RobertaConfig.from_pretrained('roberta-base')\n","# config.max_position_embeddings = max_len\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=config)\n","model.resize_token_embeddings(len(tokenizer))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if torch.cuda.device_count() > 1:\n","    model.to(device)\n","    model = torch.nn.DataParallel(model)\n","else:\n","    model.cuda()"]},{"cell_type":"code","source":["import math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","#from transformers.RobertaForSequenceClassification import RobertaSelfAttention\n","#print(model.roberta.encoder.layer[0].attention.self)\n","class RobertaDependencyAttentionMask(type(model.roberta.encoder.layer[0].attention.self)):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_value=None,\n","        output_attentions=False,\n","    ):\n","        mixed_query_layer = self.query(hidden_states)\n","\n","        # If this is instantiated as a cross-attention module, the keys\n","        # and values come from an encoder; the attention mask needs to be\n","        # such that the encoder's padding tokens are not attended to.\n","        is_cross_attention = encoder_hidden_states is not None\n","\n","        if is_cross_attention and past_key_value is not None:\n","            # reuse k,v, cross_attentions\n","            key_layer = past_key_value[0]\n","            value_layer = past_key_value[1]\n","            attention_mask = encoder_attention_mask\n","        elif is_cross_attention:\n","            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n","            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n","            attention_mask = encoder_attention_mask\n","        elif past_key_value is not None:\n","            key_layer = self.transpose_for_scores(self.key(hidden_states))\n","            value_layer = self.transpose_for_scores(self.value(hidden_states))\n","            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n","            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n","        else:\n","            key_layer = self.transpose_for_scores(self.key(hidden_states))\n","            value_layer = self.transpose_for_scores(self.value(hidden_states))\n","\n","        #print(mixed_query_layer.shape)\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        #print(mixed_query_layer.shape)\n","\n","\n","        if self.is_decoder:\n","            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n","            # Further calls to cross_attention layer can then reuse all cross-attention\n","            # key/value_states (first \"if\" case)\n","            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n","            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n","            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n","            # if encoder bi-directional self-attention `past_key_value` is always `None`\n","            past_key_value = (key_layer, value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        # print(query_layer.shape)\n","        # print(key_layer.shape)\n","        #print(attention_scores.shape)\n","        #print(hidden_states.shape)\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        #print(attention_scores.shape)\n","        #print(self.dep_mask.shape)\n","        temp_dep = self.transpose_for_scores(torch.matmul(hidden_states, self.dep_mask))\n","        #print(temp_dep.shape)\n","\n","        temp_dep = torch.matmul(temp_dep, temp_dep.transpose(-1, -2))\n","        # print(temp_dep.shape)\n","        # print(attention_scores.shape)\n","        #print(temp_dep.shape)\n","\n","        attention_scores = torch.mul(attention_scores, temp_dep)\n","\n","        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n","            seq_length = hidden_states.size()[1]\n","            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n","            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n","            distance = position_ids_l - position_ids_r\n","            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n","            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n","\n","            if self.position_embedding_type == \"relative_key\":\n","                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n","                attention_scores = attention_scores + relative_position_scores\n","            elif self.position_embedding_type == \"relative_key_query\":\n","                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n","                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n","                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n","\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        if attention_mask is not None:\n","            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n","            attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs = self.dropout(attention_probs)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","\n","        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n","\n","        if self.is_decoder:\n","            outputs = outputs + (past_key_value,)\n","        return outputs\n","\n","class RobertaDependencyForSequenceClassification(RobertaForSequenceClassification):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        for i, layer in enumerate(self.roberta.encoder.layer):\n","            attention_state = layer.attention.self.state_dict()\n","            layer.attention.self = RobertaDependencyAttentionMask(config)\n","            layer.attention.self.load_state_dict(attention_state)\n","\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        dep_mask = None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None\n","    ):\n","        for i, layer in enumerate(self.roberta.encoder.layer):\n","            layer.attention.self.dep_mask = dep_mask\n","\n","        return super().forward(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","        position_ids=position_ids,\n","        head_mask=head_mask,\n","        inputs_embeds=inputs_embeds,\n","        labels=labels,\n","        output_attentions=output_attentions,\n","        output_hidden_states=output_hidden_states,\n","        return_dict=return_dict)\n","\n","    # def dep_to_attn(self, dep_tree):\n","    #     masks = []\n","    #     for dep in dep_tree:\n","    #       dependencies = dep.split('],')\n","    #       m = np.zeros([768, 768])\n","    #       for i in range(len(dependencies)):\n","    #           for j in range(len(dependencies)):\n","    #               for x in dependencies[i]:\n","    #                   if dependencies[j][0] in x:\n","    #                       m[i][j] = 1\n","    #                   else:\n","    #                       m[i][j] = 0\n","    #       masks.append(m)\n","    #     return torch.tensor(masks, dtype=torch.float).cuda()\n","\n"],"metadata":{"id":"pgMaEcogDL0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ClearTorch()\n","\n","config = RobertaConfig.from_pretrained('roberta-base')\n","# config.max_position_embeddings = max_len\n","model = RobertaDependencyForSequenceClassification.from_pretrained('roberta-base', config=config)\n","model.resize_token_embeddings(len(tokenizer))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if torch.cuda.device_count() > 1:\n","    model.to(device)\n","    model = torch.nn.DataParallel(model)\n","else:\n","    model.cuda()"],"metadata":{"id":"BVZ0w1mPYsqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 2e-5\n","num_epoch = 1\n","\n","Train(model, train_dataloader, train_trees, learning_rate, num_epoch)"],"metadata":{"id":"J31xVW4lCvee"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0WW3gYszFL6"},"outputs":[],"source":["print(f\"Accuracy: {Eval(model, val_dataloader, val_trees)}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Roberta_Dependency.ipynb","provenance":[{"file_id":"11k2cMaY52qPZvVHfgrRojO3dmJBYh5jp","timestamp":1639535955559}]},"kernelspec":{"display_name":"Python 3 Root Install","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}